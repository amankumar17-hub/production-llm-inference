# production-llm-inference
Reference architecture for deploying distributed Llama-3/Qwen-72B inference pipelines using Ray Serve and vLLM on Kubernetes.
